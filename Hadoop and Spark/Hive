# Creata a folder name data.
mkdir data
cd data/

# Using curl to get csv file of employee data
wget https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/IBM-BD0225EN-SkillsNetwork/data/emp.csv

# Pull the hive image into your system by running the following command.
docker pull apache/hive:4.0.0-alpha-1

# Now, you will run the hive server on port 10002. You will name the server instance myhiveserver. 
We will mount the local data folder in the hive server as hive_custom_data. This would mean that the whole data folder that you created locally, 
along with anything you add in the data folder, is copied into the container under the directory hive_custom_data.

docker run -d -p 10000:10000 -p 10002:10002 --env SERVICE_NAME=hiveserver2 -v /home/project/data:/hive_custom_data --name myhiveserver apache/hive:4.0.0-alpha-1

# Now run the following command, which allows you to access beeline. This is a SQL cli where you can create, modify, delete table, and access data in the table.
docker exec -it myhiveserver beeline -u 'jdbc:hive2://localhost:10000/'



# To create a new table Employee with three columns as in the csv you downloaded - em_id, emp_name and salary, run the following command.
create table Employee(emp_id string, emp_name string, salary int) row format delimited by ',':

show tables;

# Now load the data into the table from the csv file by running the following command.
LOAD DATA INPATH '/hive_custom_data/emp.csv' INTO TABLE Employee;

# Run a querie to check the insert query
SELECT * FROM employees;

# ctrl+D to exit
